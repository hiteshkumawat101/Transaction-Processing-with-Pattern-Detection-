{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d567913-bb79-480d-b645-f8733c019bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_SOURCE=catalog\nLoading from Unity Catalog tables:\n  Transactions: datadump.test.transactions\n  Importance: datadump.test.customer_importance\nY round 1: no new keys\nSuccessfully loaded 594643 transactions and 1189286 importance records from Unity Catalog\nDEBUG: Transactions schema:\nY round 2: no new keys\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n\nDEBUG: Importance schema:\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n |-- Source: string (nullable = true)\n |-- Target: string (nullable = true)\n |-- Weight: double (nullable = true)\n |-- typeTrans: string (nullable = true)\n\nDEBUG: Sample transactions data:\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|step|customer     |age|gender|zipcodeOri|merchant     |zipMerchant|category           |amount|fraud|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|0   |'C1093826151'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|4.55  |0    |\n|0   |'C352968107' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|39.68 |0    |\n|0   |'C2054744914'|'4'|'F'   |'28007'   |'M1823072687'|'28007'    |'es_transportation'|26.89 |0    |\n|0   |'C1760612790'|'3'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|17.25 |0    |\n|0   |'C757503768' |'5'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|35.72 |0    |\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\nonly showing top 5 rows\nDEBUG: Sample importance data:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|step|customer|age |gender|zipcodeOri|merchant|zipMerchant|category|amount|fraud|Source       |Target       |Weight|typeTrans          |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55  |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72 |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\nonly showing top 5 rows\nY round 3: no new keys\nY round 4: no new keys\nY round 5: no new keys\nY round 6: no new keys\nY round 7: no new keys\nY round 8: no new keys\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060055_chunk_0_20250815_060055.csv\nX: uploaded test/transactions/20250815_060055_chunk_0_20250815_060055.csv\nY round 9: found 1 new keys\nLoading from Unity Catalog tables:\n  Transactions: datadump.test.transactions\n  Importance: datadump.test.customer_importance\nSuccessfully loaded 594643 transactions and 1189286 importance records from Unity Catalog\nDEBUG: Transactions schema:\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n\nDEBUG: Importance schema:\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n |-- Source: string (nullable = true)\n |-- Target: string (nullable = true)\n |-- Weight: double (nullable = true)\n |-- typeTrans: string (nullable = true)\n\nDEBUG: Sample transactions data:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060059_chunk_1_20250815_060059.csv\nX: uploaded test/transactions/20250815_060059_chunk_1_20250815_060059.csv\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|step|customer     |age|gender|zipcodeOri|merchant     |zipMerchant|category           |amount|fraud|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|0   |'C1093826151'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|4.55  |0    |\n|0   |'C352968107' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|39.68 |0    |\n|0   |'C2054744914'|'4'|'F'   |'28007'   |'M1823072687'|'28007'    |'es_transportation'|26.89 |0    |\n|0   |'C1760612790'|'3'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|17.25 |0    |\n|0   |'C757503768' |'5'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|35.72 |0    |\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\nonly showing top 5 rows\nDEBUG: Sample importance data:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|step|customer|age |gender|zipcodeOri|merchant|zipMerchant|category|amount|fraud|Source       |Target       |Weight|typeTrans          |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55  |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72 |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\nonly showing top 5 rows\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060101_chunk_2_20250815_060101.csv\nX: uploaded test/transactions/20250815_060101_chunk_2_20250815_060101.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060103_chunk_3_20250815_060103.csv\nX: uploaded test/transactions/20250815_060103_chunk_3_20250815_060103.csv\nDEBUG: Starting detection with 10000 transactions and 1189286 importance records\nDEBUG: Sample transactions:\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|step|customer_id  |age|gender|zipcodeori|merchant_id  |zipmerchant|category           |amount|fraud|transaction_type|transaction_id|timestamp                 |\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|0   |'C1093826151'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|4.55  |0    |PURCHASE        |0             |2025-08-15 06:00:53.199235|\n|0   |'C352968107' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|39.68 |0    |PURCHASE        |1             |2025-08-15 06:00:53.199235|\n|0   |'C2054744914'|'4'|'F'   |'28007'   |'M1823072687'|'28007'    |'es_transportation'|26.89 |0    |PURCHASE        |2             |2025-08-15 06:00:53.199235|\n|0   |'C1760612790'|'3'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|17.25 |0    |PURCHASE        |3             |2025-08-15 06:00:53.199235|\n|0   |'C757503768' |'5'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|35.72 |0    |PURCHASE        |4             |2025-08-15 06:00:53.199235|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\nonly showing top 5 rows\nDEBUG: Sample importance:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060106_chunk_4_20250815_060106.csv\nX: uploaded test/transactions/20250815_060106_chunk_4_20250815_060106.csv\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|step|customer|age |gender|zipcodeori|merchant|zipmerchant|category|amount|fraud|customer_id  |merchant_id  |weightage|transaction_type   |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55     |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72    |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\nonly showing top 5 rows\nDEBUG: Customer stats:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |total_transactions|\n+-------------+-------------+------------------+\n|'M348934600' |'C1569939854'|4                 |\n|'M348934600' |'C1156885344'|4                 |\n|'M1823072687'|'C311783581' |4                 |\n|'M348934600' |'C1597289593'|3                 |\n|'M348934600' |'C750714109' |4                 |\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant totals:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M1873032707'|2                  |\n|'M85975013'  |212                |\n|'M117188757' |1                  |\n|'M1823072687'|1747               |\n|'M1748431652'|7                  |\n+-------------+-------------------+\nonly showing top 5 rows\nDEBUG: Top 10% customers:\n+-------------+-------------+------------------+------+\n|merchant_id  |customer_id  |total_transactions|tile10|\n+-------------+-------------+------------------+------+\n|'M1053599405'|'C836424975' |2                 |1     |\n|'M1053599405'|'C354971512' |2                 |1     |\n|'M1053599405'|'C1830336096'|1                 |1     |\n|'M1053599405'|'C203802515' |1                 |1     |\n|'M1053599405'|'C1092512638'|1                 |1     |\n+-------------+-------------+------------------+------+\nonly showing top 5 rows\nDEBUG: Importance averages:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |avg_weightage     |\n+-------------+-------------+------------------+\n|NULL         |NULL         |NULL              |\n|'M348934600' |'C1569939854'|29.395384615384607|\n|'M348934600' |'C1156885344'|30.152272727272734|\n|'M1823072687'|'C311783581' |26.08876811594203 |\n|'M1823072687'|'C854479209' |24.792268041237108|\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant thresholds (10th percentile):\n+-------------+------+\n|merchant_id  |w10   |\n+-------------+------+\n|NULL         |NULL  |\n|'M1873032707'|46.36 |\n|'M2080407379'|284.41|\n|'M3697346'   |152.24|\n|'M85975013'  |23.52 |\n+-------------+------+\nonly showing top 5 rows\nDEBUG: Top 10% with weights and thresholds:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060108_chunk_5_20250815_060108.csv\nX: uploaded test/transactions/20250815_060108_chunk_5_20250815_060108.csv\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|merchant_id  |customer_id  |total_transactions|tile10|avg_weightage     |merchant_total_txns|w10  |\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|'M1053599405'|'C1830336096'|1                 |1     |57.89333333333334 |69                 |32.71|\n|'M1053599405'|'C203802515' |1                 |1     |92.00333333333333 |69                 |32.71|\n|'M1053599405'|'C1092512638'|1                 |1     |122.77000000000001|69                 |32.71|\n|'M1053599405'|'C1463833315'|1                 |1     |70.27             |69                 |32.71|\n|'M1053599405'|'C354971512' |2                 |1     |118.235           |69                 |32.71|\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\nonly showing top 5 rows\nDEBUG: MIN_TRANSACTIONS_FOR_UPGRADE = 1000\nDEBUG: Eligible customers after all filters:\n+------------+-------------+\n|merchant_id |customer_id  |\n+------------+-------------+\n|'M348934600'|'C1999090638'|\n|'M348934600'|'C1973547259'|\n|'M348934600'|'C998987490' |\n|'M348934600'|'C1227773968'|\n|'M348934600'|'C707234445' |\n+------------+-------------+\nonly showing top 5 rows\nDEBUG: Merchants that meet the transaction threshold:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M1823072687'|1747               |\n|'M348934600' |6666               |\n+-------------+-------------------+\n\nDEBUG: Customers that would be eligible without transaction threshold:\n+------------+-------------+\n|merchant_id |customer_id  |\n+------------+-------------+\n|'M840466850'|'C1781924994'|\n|'M348934600'|'C1999090638'|\n|'M348934600'|'C1973547259'|\n|'M348934600'|'C998987490' |\n|'M348934600'|'C1227773968'|\n+------------+-------------+\nonly showing top 5 rows\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060110_chunk_6_20250815_060110.csv\nX: uploaded test/transactions/20250815_060110_chunk_6_20250815_060110.csv\nDEBUG: Final detections: 22\nUploaded detections to s3://aws18082003/test/detections/20250815_060112_detections_batch_test_transactions_20250815_060055_chunk_0_20250815_060055.csv_0.json\nY: processed test/transactions/20250815_060055_chunk_0_20250815_060055.csv, detections=22\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060112_chunk_7_20250815_060112.csv\nX: uploaded test/transactions/20250815_060112_chunk_7_20250815_060112.csv\nY round 10: found 7 new keys\nLoading from Unity Catalog tables:\n  Transactions: datadump.test.transactions\n  Importance: datadump.test.customer_importance\nSuccessfully loaded 594643 transactions and 1189286 importance records from Unity Catalog\nDEBUG: Transactions schema:\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n\nDEBUG: Importance schema:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060114_chunk_8_20250815_060114.csv\nX: uploaded test/transactions/20250815_060114_chunk_8_20250815_060114.csv\nroot\n |-- step: long (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: long (nullable = true)\n |-- Source: string (nullable = true)\n |-- Target: string (nullable = true)\n |-- Weight: double (nullable = true)\n |-- typeTrans: string (nullable = true)\n\nDEBUG: Sample transactions data:\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|step|customer     |age|gender|zipcodeOri|merchant     |zipMerchant|category           |amount|fraud|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n|0   |'C1093826151'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|4.55  |0    |\n|0   |'C352968107' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|39.68 |0    |\n|0   |'C2054744914'|'4'|'F'   |'28007'   |'M1823072687'|'28007'    |'es_transportation'|26.89 |0    |\n|0   |'C1760612790'|'3'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|17.25 |0    |\n|0   |'C757503768' |'5'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|35.72 |0    |\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\nonly showing top 5 rows\nDEBUG: Sample importance data:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|step|customer|age |gender|zipcodeOri|merchant|zipMerchant|category|amount|fraud|Source       |Target       |Weight|typeTrans          |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55  |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25 |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72 |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+------+-------------------+\nonly showing top 5 rows\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060116_chunk_9_20250815_060116.csv\nX: uploaded test/transactions/20250815_060116_chunk_9_20250815_060116.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060119_chunk_10_20250815_060119.csv\nX: uploaded test/transactions/20250815_060119_chunk_10_20250815_060119.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060121_chunk_11_20250815_060121.csv\nX: uploaded test/transactions/20250815_060121_chunk_11_20250815_060121.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060123_chunk_12_20250815_060123.csv\nX: uploaded test/transactions/20250815_060123_chunk_12_20250815_060123.csv\nDEBUG: Starting detection with 10000 transactions and 1189286 importance records\nDEBUG: Sample transactions:\n+----+-------------+---+------+----------+------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|step|customer_id  |age|gender|zipcodeori|merchant_id |zipmerchant|category           |amount|fraud|transaction_type|transaction_id|timestamp                 |\n+----+-------------+---+------+----------+------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|4   |'C620327304' |'4'|'F'   |'28007'   |'M348934600'|'28007'    |'es_transportation'|20.47 |0    |PURCHASE        |10000         |2025-08-15 06:00:53.199235|\n|4   |'C1269635185'|'2'|'M'   |'28007'   |'M348934600'|'28007'    |'es_transportation'|48.9  |0    |PURCHASE        |10001         |2025-08-15 06:00:53.199235|\n|4   |'C483009385' |'3'|'F'   |'28007'   |'M348934600'|'28007'    |'es_transportation'|38.16 |0    |PURCHASE        |10002         |2025-08-15 06:00:53.199235|\n|4   |'C1761368684'|'1'|'F'   |'28007'   |'M855959430'|'28007'    |'es_hyper'         |13.25 |0    |PURCHASE        |10003         |2025-08-15 06:00:53.199235|\n|4   |'C2106115023'|'5'|'M'   |'28007'   |'M348934600'|'28007'    |'es_transportation'|14.06 |0    |PURCHASE        |10004         |2025-08-15 06:00:53.199235|\n+----+-------------+---+------+----------+------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\nonly showing top 5 rows\nDEBUG: Sample importance:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|step|customer|age |gender|zipcodeori|merchant|zipmerchant|category|amount|fraud|customer_id  |merchant_id  |weightage|transaction_type   |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55     |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72    |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\nonly showing top 5 rows\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060125_chunk_13_20250815_060125.csv\nX: uploaded test/transactions/20250815_060125_chunk_13_20250815_060125.csv\nDEBUG: Customer stats:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |total_transactions|\n+-------------+-------------+------------------+\n|'M348934600' |'C1569939854'|4                 |\n|'M1823072687'|'C854479209' |3                 |\n|'M1823072687'|'C311783581' |4                 |\n|'M348934600' |'C893276958' |3                 |\n|'M348934600' |'C1597289593'|4                 |\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant totals:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M2080407379'|7                  |\n|'M3697346'   |13                 |\n|'M85975013'  |186                |\n|'M1600850729'|56                 |\n|'M732195782' |15                 |\n+-------------+-------------------+\nonly showing top 5 rows\nDEBUG: Top 10% customers:\n+-------------+-------------+------------------+------+\n|merchant_id  |customer_id  |total_transactions|tile10|\n+-------------+-------------+------------------+------+\n|'M1053599405'|'C779872024' |2                 |1     |\n|'M1053599405'|'C1182534509'|1                 |1     |\n|'M1053599405'|'C848308923' |1                 |1     |\n|'M1053599405'|'C820456300' |1                 |1     |\n|'M1053599405'|'C353985741' |1                 |1     |\n+-------------+-------------+------------------+------+\nonly showing top 5 rows\nDEBUG: Importance averages:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |avg_weightage     |\n+-------------+-------------+------------------+\n|NULL         |NULL         |NULL              |\n|'M348934600' |'C1569939854'|29.395384615384607|\n|'M348934600' |'C1156885344\n\n*** WARNING: max output size exceeded, skipping output. ***\n\ny showing top 5 rows\nDEBUG: Top 10% customers:\n+-------------+-------------+------------------+------+\n|merchant_id  |customer_id  |total_transactions|tile10|\n+-------------+-------------+------------------+------+\n|'M1053599405'|'C1545341907'|2                 |1     |\n|'M1053599405'|'C1374607221'|2                 |1     |\n|'M1053599405'|'C1519265307'|1                 |1     |\n|'M1053599405'|'C1304285526'|1                 |1     |\n|'M1053599405'|'C998987490' |1                 |1     |\n+-------------+-------------+------------------+------+\nonly showing top 5 rows\nDEBUG: Importance averages:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |avg_weightage     |\n+-------------+-------------+------------------+\n|NULL         |NULL         |NULL              |\n|'M348934600' |'C1569939854'|29.395384615384607|\n|'M348934600' |'C1156885344'|30.152272727272734|\n|'M1823072687'|'C311783581' |26.08876811594203 |\n|'M1823072687'|'C854479209' |24.792268041237108|\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant thresholds (10th percentile):\n+-------------+------+\n|merchant_id  |w10   |\n+-------------+------+\n|NULL         |NULL  |\n|'M1873032707'|46.36 |\n|'M2080407379'|284.41|\n|'M3697346'   |152.24|\n|'M85975013'  |23.52 |\n+-------------+------+\nonly showing top 5 rows\nDEBUG: Top 10% with weights and thresholds:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060205_chunk_31_20250815_060205.csv\nX: uploaded test/transactions/20250815_060205_chunk_31_20250815_060205.csv\n+-------------+-------------+------------------+------+-----------------+-------------------+-----+\n|merchant_id  |customer_id  |total_transactions|tile10|avg_weightage    |merchant_total_txns|w10  |\n+-------------+-------------+------------------+------+-----------------+-------------------+-----+\n|'M1053599405'|'C1519265307'|1                 |1     |137.94           |58                 |32.71|\n|'M1053599405'|'C1374607221'|2                 |1     |101.775          |58                 |32.71|\n|'M1053599405'|'C1304285526'|1                 |1     |19.85            |58                 |32.71|\n|'M1053599405'|'C998987490' |1                 |1     |90.02666666666669|58                 |32.71|\n|'M1053599405'|'C457497998' |1                 |1     |88.78500000000001|58                 |32.71|\n+-------------+-------------+------------------+------+-----------------+-------------------+-----+\nonly showing top 5 rows\nDEBUG: MIN_TRANSACTIONS_FOR_UPGRADE = 1000\nDEBUG: Eligible customers after all filters:\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M348934600' |'C586830923' |\n|'M348934600' |'C1599273075'|\n|'M348934600' |'C1903005844'|\n|'M1823072687'|'C983636862' |\n|'M348934600' |'C706685788' |\n+-------------+-------------+\nonly showing top 5 rows\nDEBUG: Merchants that meet the transaction threshold:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M1823072687'|2996               |\n|'M348934600' |5514               |\n+-------------+-------------------+\n\nDEBUG: Customers that would be eligible without transaction threshold:\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M97925176'  |'C1387070447'|\n|'M348934600' |'C586830923' |\n|'M348934600' |'C1599273075'|\n|'M1600850729'|'C281893917' |\n|'M2080407379'|'C1433132752'|\n+-------------+-------------+\nonly showing top 5 rows\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060208_chunk_32_20250815_060207.csv\nX: uploaded test/transactions/20250815_060208_chunk_32_20250815_060207.csv\nDEBUG: Final detections: 16\nUploaded detections to s3://aws18082003/test/detections/20250815_060209_detections_batch_test_transactions_20250815_060108_chunk_5_20250815_060108.csv_0.json\nY: processed test/transactions/20250815_060108_chunk_5_20250815_060108.csv, detections=16\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060209_chunk_33_20250815_060209.csv\nX: uploaded test/transactions/20250815_060209_chunk_33_20250815_060209.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060211_chunk_34_20250815_060211.csv\nX: uploaded test/transactions/20250815_060211_chunk_34_20250815_060211.csv\nDEBUG: Starting detection with 10000 transactions and 1189286 importance records\nDEBUG: Sample transactions:\n+----+-------------+---+------+----------+-------------+-----------+----------------------+------+-----+----------------+--------------+--------------------------+\n|step|customer_id  |age|gender|zipcodeori|merchant_id  |zipmerchant|category              |amount|fraud|transaction_type|transaction_id|timestamp                 |\n+----+-------------+---+------+----------+-------------+-----------+----------------------+------+-----+----------------+--------------+--------------------------+\n|22  |'C1978250683'|'3'|'F'   |'28007'   |'M1535107174'|'28007'    |'es_wellnessandbeauty'|51.19 |1    |PURCHASE        |60000         |2025-08-15 06:00:53.199235|\n|22  |'C606355103' |'4'|'F'   |'28007'   |'M348934600' |'28007'    |'es_transportation'   |3.84  |0    |PURCHASE        |60001         |2025-08-15 06:00:53.199235|\n|22  |'C1248026439'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'   |26.84 |0    |PURCHASE        |60002         |2025-08-15 06:00:53.199235|\n|22  |'C1809615650'|'4'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'   |11.14 |0    |PURCHASE        |60003         |2025-08-15 06:00:53.199235|\n|22  |'C592766342' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'   |24.2  |0    |PURCHASE        |60004         |2025-08-15 06:00:53.199235|\n+----+-------------+---+------+----------+-------------+-----------+----------------------+------+-----+----------------+--------------+--------------------------+\nonly showing top 5 rows\nDEBUG: Sample importance:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|step|customer|age |gender|zipcodeori|merchant|zipmerchant|category|amount|fraud|customer_id  |merchant_id  |weightage|transaction_type   |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55     |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72    |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\nonly showing top 5 rows\nDEBUG: Customer stats:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |total_transactions|\n+-------------+-------------+------------------+\n|'M348934600' |'C1569939854'|4                 |\n|'M1823072687'|'C381250974' |3                 |\n|'M348934600' |'C893276958' |3                 |\n|'M85975013'  |'C1603928569'|1                 |\n|'M1823072687'|'C1467832188'|3                 |\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant totals:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M3697346'   |12                 |\n|'M85975013'  |306                |\n|'M1600850729'|54                 |\n|'M732195782' |13                 |\n|'M1823072687'|3195               |\n+-------------+-------------------+\nonly showing top 5 rows\nDEBUG: Top 10% customers:\n+-------------+-------------+------------------+------+\n|merchant_id  |customer_id  |total_transactions|tile10|\n+-------------+-------------+------------------+------+\n|'M1053599405'|'C776775180' |2                 |1     |\n|'M1053599405'|'C1669649966'|1                 |1     |\n|'M1053599405'|'C1421006824'|1                 |1     |\n|'M1053599405'|'C597262762' |1                 |1     |\n|'M1053599405'|'C362857866' |1                 |1     |\n+-------------+-------------+------------------+------+\nonly showing top 5 rows\nDEBUG: Importance averages:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060214_chunk_35_20250815_060214.csv\nX: uploaded test/transactions/20250815_060214_chunk_35_20250815_060214.csv\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |avg_weightage     |\n+-------------+-------------+------------------+\n|NULL         |NULL         |NULL              |\n|'M348934600' |'C1569939854'|29.395384615384607|\n|'M348934600' |'C1156885344'|30.152272727272734|\n|'M1823072687'|'C311783581' |26.08876811594203 |\n|'M1823072687'|'C854479209' |24.792268041237108|\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant thresholds (10th percentile):\n+-------------+------+\n|merchant_id  |w10   |\n+-------------+------+\n|NULL         |NULL  |\n|'M1873032707'|46.36 |\n|'M2080407379'|284.41|\n|'M3697346'   |152.24|\n|'M85975013'  |23.52 |\n+-------------+------+\nonly showing top 5 rows\nDEBUG: Top 10% with weights and thresholds:\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|merchant_id  |customer_id  |total_transactions|tile10|avg_weightage     |merchant_total_txns|w10  |\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|'M1053599405'|'C1669649966'|1                 |1     |111.91499999999999|61                 |32.71|\n|'M1053599405'|'C1421006824'|1                 |1     |91.54833333333333 |61                 |32.71|\n|'M1053599405'|'C597262762' |1                 |1     |147.6875          |61                 |32.71|\n|'M1053599405'|'C362857866' |1                 |1     |60.626666666666665|61                 |32.71|\n|'M1053599405'|'C109947606' |1                 |1     |70.37875          |61                 |32.71|\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\nonly showing top 5 rows\nDEBUG: MIN_TRANSACTIONS_FOR_UPGRADE = 1000\nDEBUG: Eligible customers after all filters:\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M348934600' |'C145901862' |\n|'M348934600' |'C350348759' |\n|'M348934600' |'C1057813963'|\n|'M348934600' |'C1861613802'|\n|'M1823072687'|'C983636862' |\n+-------------+-------------+\nonly showing top 5 rows\nDEBUG: Merchants that meet the transaction threshold:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060216_chunk_36_20250815_060216.csv\nX: uploaded test/transactions/20250815_060216_chunk_36_20250815_060216.csv\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M1823072687'|3195               |\n|'M348934600' |5304               |\n+-------------+-------------------+\n\nDEBUG: Customers that would be eligible without transaction threshold:\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M348934600' |'C145901862' |\n|'M348934600' |'C350348759' |\n|'M348934600' |'C1057813963'|\n|'M348934600' |'C1861613802'|\n|'M1823072687'|'C983636862' |\n+-------------+-------------+\nonly showing top 5 rows\nDEBUG: Final detections: 20\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060218_chunk_37_20250815_060218.csv\nX: uploaded test/transactions/20250815_060218_chunk_37_20250815_060218.csv\nUploaded detections to s3://aws18082003/test/detections/20250815_060218_detections_batch_test_transactions_20250815_060110_chunk_6_20250815_060110.csv_0.json\nY: processed test/transactions/20250815_060110_chunk_6_20250815_060110.csv, detections=20\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060220_chunk_38_20250815_060220.csv\nX: uploaded test/transactions/20250815_060220_chunk_38_20250815_060220.csv\nDEBUG: Starting detection with 10000 transactions and 1189286 importance records\nDEBUG: Sample transactions:\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|step|customer_id  |age|gender|zipcodeori|merchant_id  |zipmerchant|category           |amount|fraud|transaction_type|transaction_id|timestamp                 |\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\n|26  |'C638838388' |'2'|'F'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|13.73 |0    |PURCHASE        |70000         |2025-08-15 06:00:53.199235|\n|26  |'C1750362802'|'5'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|24.28 |0    |PURCHASE        |70001         |2025-08-15 06:00:53.199235|\n|26  |'C430171917' |'2'|'M'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|37.56 |0    |PURCHASE        |70002         |2025-08-15 06:00:53.199235|\n|26  |'C567897093' |'2'|'M'   |'28007'   |'M1823072687'|'28007'    |'es_transportation'|29.58 |0    |PURCHASE        |70003         |2025-08-15 06:00:53.199235|\n|26  |'C59912595'  |'2'|'F'   |'28007'   |'M348934600' |'28007'    |'es_transportation'|57.16 |0    |PURCHASE        |70004         |2025-08-15 06:00:53.199235|\n+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+----------------+--------------+--------------------------+\nonly showing top 5 rows\nDEBUG: Sample importance:\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|step|customer|age |gender|zipcodeori|merchant|zipmerchant|category|amount|fraud|customer_id  |merchant_id  |weightage|transaction_type   |\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1093826151'|'M348934600' |4.55     |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C352968107' |'M348934600' |39.68    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C2054744914'|'M1823072687'|26.89    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C1760612790'|'M348934600' |17.25    |'es_transportation'|\n|NULL|NULL    |NULL|NULL  |NULL      |NULL    |NULL       |NULL    |NULL  |0    |'C757503768' |'M348934600' |35.72    |'es_transportation'|\n+----+--------+----+------+----------+--------+-----------+--------+------+-----+-------------+-------------+---------+-------------------+\nonly showing top 5 rows\nDEBUG: Customer stats:\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |total_transactions|\n+-------------+-------------+------------------+\n|'M348934600' |'C1575367607'|3                 |\n|'M1823072687'|'C854479209' |4                 |\n|'M348934600' |'C1569939854'|4                 |\n|'M348934600' |'C893276958' |3                 |\n|'M480139044' |'C276409392' |1                 |\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant totals:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M3697346'   |8                  |\n|'M85975013'  |304                |\n|'M1600850729'|46                 |\n|'M732195782' |18                 |\n|'M1823072687'|3282               |\n+-------------+-------------------+\nonly showing top 5 rows\nDEBUG: Top 10% customers:\n+-------------+-------------+------------------+------+\n|merchant_id  |customer_id  |total_transactions|tile10|\n+-------------+-------------+------------------+------+\n|'M1053599405'|'C1058785965'|2                 |1     |\n|'M1053599405'|'C1962137244'|2                 |1     |\n|'M1053599405'|'C1954938610'|1                 |1     |\n|'M1053599405'|'C1495681901'|1                 |1     |\n|'M1053599405'|'C891364505' |1                 |1     |\n+-------------+-------------+------------------+------+\nonly showing top 5 rows\nDEBUG: Importance averages:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060223_chunk_39_20250815_060223.csv\nX: uploaded test/transactions/20250815_060223_chunk_39_20250815_060223.csv\n+-------------+-------------+------------------+\n|merchant_id  |customer_id  |avg_weightage     |\n+-------------+-------------+------------------+\n|NULL         |NULL         |NULL              |\n|'M348934600' |'C1569939854'|29.395384615384607|\n|'M348934600' |'C1156885344'|30.152272727272734|\n|'M1823072687'|'C311783581' |26.08876811594203 |\n|'M1823072687'|'C854479209' |24.792268041237108|\n+-------------+-------------+------------------+\nonly showing top 5 rows\nDEBUG: Merchant thresholds (10th percentile):\n+-------------+------+\n|merchant_id  |w10   |\n+-------------+------+\n|NULL         |NULL  |\n|'M1873032707'|46.36 |\n|'M2080407379'|284.41|\n|'M3697346'   |152.24|\n|'M85975013'  |23.52 |\n+-------------+------+\nonly showing top 5 rows\nDEBUG: Top 10% with weights and thresholds:\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|merchant_id  |customer_id  |total_transactions|tile10|avg_weightage     |merchant_total_txns|w10  |\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\n|'M1053599405'|'C1954938610'|1                 |1     |3.71              |82                 |32.71|\n|'M1053599405'|'C1495681901'|1                 |1     |152.305           |82                 |32.71|\n|'M1053599405'|'C891364505' |1                 |1     |138.17999999999998|82                 |32.71|\n|'M1053599405'|'C808326652' |1                 |1     |83.14857142857143 |82                 |32.71|\n|'M1053599405'|'C1058785965'|2                 |1     |146.02499999999998|82                 |32.71|\n+-------------+-------------+------------------+------+------------------+-------------------+-----+\nonly showing top 5 rows\nDEBUG: MIN_TRANSACTIONS_FOR_UPGRADE = 1000\nDEBUG: Eligible customers after all filters:\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M348934600' |'C2100602051'|\n|'M348934600' |'C2034859281'|\n|'M1823072687'|'C52299933'  |\n|'M1823072687'|'C1734287915'|\n|'M1823072687'|'C719449879' |\n+-------------+-------------+\nonly showing top 5 rows\nDEBUG: Merchants that meet the transaction threshold:\n+-------------+-------------------+\n|merchant_id  |merchant_total_txns|\n+-------------+-------------------+\n|'M1823072687'|3282               |\n|'M348934600' |5133               |\n+-------------+-------------------+\n\nDEBUG: Customers that would be eligible without transaction threshold:\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060225_chunk_40_20250815_060225.csv\nX: uploaded test/transactions/20250815_060225_chunk_40_20250815_060225.csv\n+-------------+-------------+\n|merchant_id  |customer_id  |\n+-------------+-------------+\n|'M1872033263'|'C1970738826'|\n|'M1053599405'|'C1954938610'|\n|'M840466850' |'C460167545' |\n|'M348934600' |'C2100602051'|\n|'M348934600' |'C2034859281'|\n+-------------+-------------+\nonly showing top 5 rows\nDEBUG: Final detections: 9\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060227_chunk_41_20250815_060227.csv\nX: uploaded test/transactions/20250815_060227_chunk_41_20250815_060227.csv\nUploaded detections to s3://aws18082003/test/detections/20250815_060227_detections_batch_test_transactions_20250815_060112_chunk_7_20250815_060112.csv_0.json\nY: processed test/transactions/20250815_060112_chunk_7_20250815_060112.csv, detections=9\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060229_chunk_42_20250815_060229.csv\nX: uploaded test/transactions/20250815_060229_chunk_42_20250815_060229.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060232_chunk_43_20250815_060232.csv\nX: uploaded test/transactions/20250815_060232_chunk_43_20250815_060232.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060234_chunk_44_20250815_060234.csv\nX: uploaded test/transactions/20250815_060234_chunk_44_20250815_060234.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060236_chunk_45_20250815_060236.csv\nX: uploaded test/transactions/20250815_060236_chunk_45_20250815_060236.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060238_chunk_46_20250815_060238.csv\nX: uploaded test/transactions/20250815_060238_chunk_46_20250815_060238.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060240_chunk_47_20250815_060240.csv\nX: uploaded test/transactions/20250815_060240_chunk_47_20250815_060240.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060242_chunk_48_20250815_060242.csv\nX: uploaded test/transactions/20250815_060242_chunk_48_20250815_060242.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060245_chunk_49_20250815_060245.csv\nX: uploaded test/transactions/20250815_060245_chunk_49_20250815_060245.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060247_chunk_50_20250815_060247.csv\nX: uploaded test/transactions/20250815_060247_chunk_50_20250815_060247.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060249_chunk_51_20250815_060249.csv\nX: uploaded test/transactions/20250815_060249_chunk_51_20250815_060249.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060251_chunk_52_20250815_060251.csv\nX: uploaded test/transactions/20250815_060251_chunk_52_20250815_060251.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060254_chunk_53_20250815_060254.csv\nX: uploaded test/transactions/20250815_060254_chunk_53_20250815_060254.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060256_chunk_54_20250815_060256.csv\nX: uploaded test/transactions/20250815_060256_chunk_54_20250815_060256.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060258_chunk_55_20250815_060258.csv\nX: uploaded test/transactions/20250815_060258_chunk_55_20250815_060258.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060300_chunk_56_20250815_060300.csv\nX: uploaded test/transactions/20250815_060300_chunk_56_20250815_060300.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060303_chunk_57_20250815_060302.csv\nX: uploaded test/transactions/20250815_060303_chunk_57_20250815_060302.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060305_chunk_58_20250815_060305.csv\nX: uploaded test/transactions/20250815_060305_chunk_58_20250815_060305.csv\nUploaded chunk to s3://aws18082003/test/transactions/20250815_060307_chunk_59_20250815_060307.csv\nX: uploaded test/transactions/20250815_060307_chunk_59_20250815_060307.csv\nTransactions in S3:\n - test/transactions/20250815_060055_chunk_0_20250815_060055.csv\n - test/transactions/20250815_060059_chunk_1_20250815_060059.csv\n - test/transactions/20250815_060101_chunk_2_20250815_060101.csv\n - test/transactions/20250815_060103_chunk_3_20250815_060103.csv\n - test/transactions/20250815_060106_chunk_4_20250815_060106.csv\n - test/transactions/20250815_060108_chunk_5_20250815_060108.csv\n - test/transactions/20250815_060110_chunk_6_20250815_060110.csv\n - test/transactions/20250815_060112_chunk_7_20250815_060112.csv\n - test/transactions/20250815_060114_chunk_8_20250815_060114.csv\n - test/transactions/20250815_060116_chunk_9_20250815_060116.csv\n\nDetections in S3:\n - test/detections/20250815_060112_detections_batch_test_transactions_20250815_060055_chunk_0_20250815_060055.csv_0.json\n - test/detections/20250815_060131_detections_batch_test_transactions_20250815_060059_chunk_1_20250815_060059.csv_0.json\n - test/detections/20250815_060141_detections_batch_test_transactions_20250815_060101_chunk_2_20250815_060101.csv_0.json\n - test/detections/20250815_060151_detections_batch_test_transactions_20250815_060103_chunk_3_20250815_060103.csv_0.json\n - test/detections/20250815_060200_detections_batch_test_transactions_20250815_060106_chunk_4_20250815_060106.csv_0.json\n - test/detections/20250815_060209_detections_batch_test_transactions_20250815_060108_chunk_5_20250815_060108.csv_0.json\n - test/detections/20250815_060218_detections_batch_test_transactions_20250815_060110_chunk_6_20250815_060110.csv_0.json\n - test/detections/20250815_060227_detections_batch_test_transactions_20250815_060112_chunk_7_20250815_060112.csv_0.json\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "#  %md\n",
    "#  # Transaction Processing (S3/DBFS/Workspace/GDrive Sources)\n",
    "#  \n",
    "#  This notebook implements mechanism X and Y:\n",
    "#  - X: every-second chunking of transactions (10,000 rows per chunk) to S3\n",
    "#  - Y: polling S3 for newly arrived chunks, detecting PatId1, and writing detections in batches of 50 to S3\n",
    "#  \n",
    "#  Optional: downloads inputs from Google Drive into the cluster if configured.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Setup\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# If not already present on the cluster\n",
    "#  %pip install boto3 psycopg2-binary pytz\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Set, Tuple\n",
    "import threading\n",
    "\n",
    "import pytz\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    avg,\n",
    "    sum as spark_sum,\n",
    "    count as spark_count,\n",
    "    desc,\n",
    "    ntile,\n",
    "    expr,\n",
    "    current_timestamp,\n",
    "    monotonically_increasing_id,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import execute_values\n",
    "except Exception:\n",
    "    psycopg2 = None\n",
    "\n",
    "# ---- Config (edit these or set env vars) ----\n",
    "AWS_ACCESS_KEY_ID = \"test\"\n",
    "AWS_SECRET_ACCESS_KEY = \"t/R93uho8pGwJB6rnt0kL9\"\n",
    "AWS_REGION = \"eu-north-1\"\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"eu-north-1\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"aws18082003\")\n",
    "S3_TRANSACTIONS_FOLDER = os.getenv(\"S3_TRANSACTIONS_FOLDER\", \"test/transactions/\")\n",
    "S3_DETECTIONS_FOLDER = os.getenv(\"S3_DETECTIONS_FOLDER\", \"test/detections/\")\n",
    "\n",
    "# Source selection: 's3', 'volume', 'dbfs', 'workspace', 'catalog', or 'gdrive'\n",
    "INPUT_SOURCE = os.getenv(\"INPUT_SOURCE\", \"catalog\")  # Changed default to catalog since that's working\n",
    "\n",
    "# S3 URIs for input CSVs (if INPUT_SOURCE == 's3')\n",
    "INPUT_CUSTOMER_IMPORTANCE_S3 = os.getenv(\"INPUT_CUSTOMER_IMPORTANCE_S3\", \"s3a://aws18082003/CustomerImportance.csv\")\n",
    "INPUT_TRANSACTIONS_S3 = os.getenv(\"INPUT_TRANSACTIONS_S3\", \"s3a://aws18082003/transactions.csv\")\n",
    "\n",
    "# Unity Catalog Volume paths (upload files to a UC Volume if using 'volume')\n",
    "TRANSACTIONS_CSV_VOLUME = os.getenv(\"TRANSACTIONS_CSV_VOLUME\", \"\")\n",
    "CUSTOMER_IMPORTANCE_CSV_VOLUME = os.getenv(\"CUSTOMER_IMPORTANCE_CSV_VOLUME\", \"\")\n",
    "\n",
    "# DBFS file inputs (if INPUT_SOURCE == 'dbfs')\n",
    "TRANSACTIONS_CSV_DBFS = os.getenv(\"TRANSACTIONS_CSV_DBFS\", \"dbfs:/FileStore/transactions.csv\")\n",
    "CUSTOMER_IMPORTANCE_CSV_DBFS = os.getenv(\"CUSTOMER_IMPORTANCE_CSV_DBFS\", \"dbfs:/FileStore/CustomerImportance.csv\")\n",
    "\n",
    "# Databricks Workspace absolute paths (if INPUT_SOURCE == 'workspace')\n",
    "WORKSPACE_TRANSACTIONS_PATH = os.getenv(\"WORKSPACE_TRANSACTIONS_PATH\", \"/Workspace/Users/you@example.com/transactions.csv\")\n",
    "WORKSPACE_IMPORTANCE_PATH = os.getenv(\"WORKSPACE_IMPORTANCE_PATH\", \"/Workspace/Users/you@example.com/CustomerImportance.csv\")\n",
    "\n",
    "# Google Drive inputs (if INPUT_SOURCE == 'gdrive') - using mounted path\n",
    "GDRIVE_MOUNT_PATH = os.getenv(\"GDRIVE_MOUNT_PATH\", \"/mnt/google_drive\")\n",
    "GDRIVE_TRANSACTIONS_FILENAME = os.getenv(\"GDRIVE_TRANSACTIONS_FILENAME\", \"transactions.csv\")\n",
    "GDRIVE_IMPORTANCE_FILENAME = os.getenv(\"GDRIVE_IMPORTANCE_FILENAME\", \"CustomerImportance.csv\")\n",
    "\n",
    "# Unity Catalog paths (if INPUT_SOURCE == 'catalog')\n",
    "\n",
    "CATALOG_TRANSACTIONS_PATH = 'datadump.test.transactions'\n",
    "CATALOG_IMPORTANCE_PATH = 'datadump.test.customer_importance'\n",
    "\n",
    "# Processing\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"10000\"))\n",
    "\n",
    "DETECTION_BATCH_SIZE = int(os.getenv(\"DETECTION_BATCH_SIZE\", \"50\"))\n",
    "MIN_TRANSACTIONS_FOR_UPGRADE = int(os.getenv(\"MIN_TRANSACTIONS_FOR_UPGRADE\", \"1000\"))\n",
    "\n",
    "# X: sleep between chunk uploads (seconds)\n",
    "X_SLEEP_SECONDS = float(os.getenv(\"X_SLEEP_SECONDS\", \"1\"))\n",
    "\n",
    "# Y: polling behavior\n",
    "Y_POLL_INTERVAL_SECS = float(os.getenv(\"Y_POLL_INTERVAL_SECS\", \"1\"))\n",
    "Y_MAX_POLL_ROUNDS = int(os.getenv(\"Y_MAX_POLL_ROUNDS\", \"10\"))  # safety to avoid infinite loop in demos\n",
    "\n",
    "# Postgres (optional)\n",
    "PG_HOST = os.getenv(\"PG_HOST\", \"\")\n",
    "PG_PORT = int(os.getenv(\"PG_PORT\", \"5432\"))\n",
    "PG_DB = os.getenv(\"PG_DB\", \"\")\n",
    "PG_USER = os.getenv(\"PG_USER\", \"\")\n",
    "PG_PASSWORD = os.getenv(\"PG_PASSWORD\", \"\")\n",
    "PG_TABLE_KEYS = os.getenv(\"PG_TABLE_KEYS\", \"processed_s3_keys\")\n",
    "\n",
    "IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "# Run selection\n",
    "RUN_MODE = os.getenv(\"RUN_MODE\", \"both\")  # \"x\" | \"y\" | \"both\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Utilities\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def _to_s3_uri(s3a_uri: str) -> str:\n",
    "    return s3a_uri.replace(\"s3a://\", \"s3://\")\n",
    "\n",
    "\n",
    "def _read_csv_from_s3_via_boto(s3_uri: str) -> pd.DataFrame:\n",
    "    uri = _to_s3_uri(s3_uri)\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    p = urlparse(uri)\n",
    "    bucket, key = p.netloc, p.path.lstrip(\"/\")\n",
    "    client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name=AWS_REGION,\n",
    "    )\n",
    "    obj = client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.StringIO(obj[\"Body\"].read().decode(\"utf-8\")))\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## S3 Client\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class S3Client:\n",
    "    def __init__(self):\n",
    "        self.s3_client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION,\n",
    "        )\n",
    "        self.bucket = S3_BUCKET_NAME\n",
    "        self.tx_prefix = S3_TRANSACTIONS_FOLDER\n",
    "        self.det_prefix = S3_DETECTIONS_FOLDER\n",
    "\n",
    "    def create_bucket_if_not_exists(self) -> bool:\n",
    "        try:\n",
    "            self.s3_client.head_bucket(Bucket=self.bucket)\n",
    "            return True\n",
    "        except Exception:\n",
    "            params = {\"Bucket\": self.bucket}\n",
    "            if AWS_REGION != \"us-east-1\":\n",
    "                params[\"CreateBucketConfiguration\"] = {\"LocationConstraint\": AWS_REGION}\n",
    "            self.s3_client.create_bucket(**params)\n",
    "            return True\n",
    "\n",
    "    def upload_transaction_chunk(self, df: pd.DataFrame, chunk_id: str) -> str:\n",
    "        csv_data = df.to_csv(index=False)\n",
    "        key = f\"{self.tx_prefix}{datetime.now().strftime('%Y%m%d_%H%M%S')}_{chunk_id}.csv\"\n",
    "        self.s3_client.put_object(\n",
    "            Bucket=self.bucket,\n",
    "            Key=key,\n",
    "            Body=csv_data.encode(\"utf-8\"),\n",
    "            ContentType=\"text/csv\",\n",
    "        )\n",
    "        print(f\"Uploaded chunk to s3://{self.bucket}/{key}\")\n",
    "        return key\n",
    "\n",
    "    def upload_detections_batch(self, detections: list, batch_id: str) -> str:\n",
    "        body = json.dumps(detections, indent=2, default=str).encode(\"utf-8\")\n",
    "        key = f\"{self.det_prefix}{datetime.now().strftime('%Y%m%d_%H%M%S')}_detections_batch_{batch_id}.json\"\n",
    "        self.s3_client.put_object(\n",
    "            Bucket=self.bucket,\n",
    "            Key=key,\n",
    "            Body=body,\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "        print(f\"Uploaded detections to s3://{self.bucket}/{key}\")\n",
    "        return key\n",
    "\n",
    "    def list_keys(self, prefix: str) -> List[str]:\n",
    "        resp = self.s3_client.list_objects_v2(Bucket=self.bucket, Prefix=prefix)\n",
    "        return [o[\"Key\"] for o in (resp.get(\"Contents\", []) or [])]\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Optional Postgres tracking\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def pg_available() -> bool:\n",
    "    return bool(PG_HOST and PG_DB and PG_USER and PG_PASSWORD and psycopg2 is not None)\n",
    "\n",
    "\n",
    "def pg_connect():\n",
    "    if not pg_available():\n",
    "        return None\n",
    "    return psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        database=PG_DB,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD,\n",
    "    )\n",
    "\n",
    "\n",
    "def pg_ensure_tables():\n",
    "    conn = pg_connect()\n",
    "    if conn is None:\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PG_TABLE_KEYS} (\n",
    "            s3_key TEXT PRIMARY KEY,\n",
    "            processed_at TIMESTAMPTZ DEFAULT NOW()\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def pg_load_processed_keys() -> Set[str]:\n",
    "    conn = pg_connect()\n",
    "    if conn is None:\n",
    "        return set()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT s3_key FROM {PG_TABLE_KEYS}\")\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return {r[0] for r in rows}\n",
    "\n",
    "\n",
    "def pg_mark_keys_processed(keys: List[str]):\n",
    "    if not keys:\n",
    "        return\n",
    "    conn = pg_connect()\n",
    "    if conn is None:\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    execute_values(cur, f\"INSERT INTO {PG_TABLE_KEYS} (s3_key) VALUES %s ON CONFLICT DO NOTHING\", [(k,) for k in keys])\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Normalization helpers\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def normalize_txns(df: DataFrame) -> DataFrame:\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    df = df.toDF(*cols)\n",
    "    renames = {\n",
    "        \"merchant\": \"merchant_id\",\n",
    "        \"merchantid\": \"merchant_id\",\n",
    "        \"customer\": \"customer_id\",\n",
    "        \"customerid\": \"customer_id\",\n",
    "        \"user\": \"customer_id\",\n",
    "        \"userid\": \"customer_id\",\n",
    "        \"trans_id\": \"transaction_id\",\n",
    "        \"transactionid\": \"transaction_id\",\n",
    "        \"txnid\": \"transaction_id\",\n",
    "        \"txntype\": \"transaction_type\",\n",
    "        \"type\": \"transaction_type\",\n",
    "    }\n",
    "    for src, dst in renames.items():\n",
    "        if src in df.columns:\n",
    "            df = df.withColumnRenamed(src, dst)\n",
    "    if \"merchant_id\" not in df.columns:\n",
    "        raise ValueError(f\"merchant_id not found. Available: {df.columns}\")\n",
    "    if \"customer_id\" not in df.columns:\n",
    "        df = df.withColumn(\"customer_id\", lit(\"\"))\n",
    "    if \"amount\" not in df.columns:\n",
    "        df = df.withColumn(\"amount\", lit(0.0).cast(\"double\"))\n",
    "    else:\n",
    "        df = df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "    if \"transaction_type\" not in df.columns:\n",
    "        df = df.withColumn(\"transaction_type\", lit(\"PURCHASE\"))\n",
    "    if \"transaction_id\" not in df.columns:\n",
    "        df = df.withColumn(\"transaction_id\", monotonically_increasing_id().cast(\"string\"))\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        df = df.withColumn(\"timestamp\", current_timestamp().cast(\"string\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_importance(df: DataFrame) -> DataFrame:\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    df = df.toDF(*cols)\n",
    "    renames = {\n",
    "        \"source\": \"customer_id\",\n",
    "        \"target\": \"merchant_id\",\n",
    "        \"typetrans\": \"transaction_type\",\n",
    "        \"type\": \"transaction_type\",\n",
    "        \"weight\": \"weightage\",\n",
    "    }\n",
    "    for src, dst in renames.items():\n",
    "        if src in df.columns:\n",
    "            df = df.withColumnRenamed(src, dst)\n",
    "    if \"weightage\" in df.columns:\n",
    "        df = df.withColumn(\"weightage\", col(\"weightage\").cast(\"double\"))\n",
    "    for req, default in [\n",
    "        (\"customer_id\", lit(\"\")),\n",
    "        (\"merchant_id\", lit(\"\")),\n",
    "        (\"transaction_type\", lit(\"PURCHASE\")),\n",
    "        (\"weightage\", lit(0.1)),\n",
    "    ]:\n",
    "        if req not in df.columns:\n",
    "            df = df.withColumn(req, default)\n",
    "    return df\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Input loading (S3/DBFS/Workspace/Volume/GDrive)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def load_from_gdrive(spark: SparkSession) -> tuple[DataFrame, DataFrame]:\n",
    "    # Use the mounted Google Drive path in Databricks\n",
    "    tx_path = f\"{GDRIVE_MOUNT_PATH}/{GDRIVE_TRANSACTIONS_FILENAME}\"\n",
    "    im_path = f\"{GDRIVE_MOUNT_PATH}/{GDRIVE_IMPORTANCE_FILENAME}\"\n",
    "    \n",
    "    # List files in the mounted directory to verify\n",
    "    print(f\"Listing files in {GDRIVE_MOUNT_PATH}:\")\n",
    "    try:\n",
    "        files = dbutils.fs.ls(GDRIVE_MOUNT_PATH)\n",
    "        for file in files:\n",
    "            print(f\"  - {file.name} (size: {file.size} bytes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not list files in {GDRIVE_MOUNT_PATH}: {e}\")\n",
    "    \n",
    "    print(f\"Loading transactions from: {tx_path}\")\n",
    "    print(f\"Loading importance from: {im_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Try to read entire files using dbutils.fs.read()\n",
    "        print(\"Attempting to read files using dbutils.fs.read()...\")\n",
    "        \n",
    "        # Get file sizes first\n",
    "        tx_size = dbutils.fs.head(tx_path, 1)  # Just check if accessible\n",
    "        im_size = dbutils.fs.head(im_path, 1)\n",
    "        \n",
    "        # Read entire files\n",
    "        tx_content = dbutils.fs.read(tx_path)\n",
    "        im_content = dbutils.fs.read(im_path)\n",
    "        \n",
    "        # Convert to pandas DataFrames\n",
    "        import io\n",
    "        txns_pd = pd.read_csv(io.StringIO(tx_content))\n",
    "        importance_pd = pd.read_csv(io.StringIO(im_content))\n",
    "        \n",
    "        # Convert to Spark DataFrames\n",
    "        txns = spark.createDataFrame(txns_pd)\n",
    "        importance = spark.createDataFrame(importance_pd)\n",
    "        \n",
    "        print(f\"Successfully loaded {txns.count()} transactions and {importance.count()} importance records\")\n",
    "        return txns, importance\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "        print(\"Trying Method 2: Copy to DBFS first...\")\n",
    "        \n",
    "        try:\n",
    "            # Method 2: Copy files to DBFS and then read\n",
    "            dbfs_tx_path = f\"dbfs:/FileStore/{GDRIVE_TRANSACTIONS_FILENAME}\"\n",
    "            dbfs_im_path = f\"dbfs:/FileStore/{GDRIVE_IMPORTANCE_FILENAME}\"\n",
    "            \n",
    "            print(f\"Copying {tx_path} to {dbfs_tx_path}\")\n",
    "            dbutils.fs.cp(tx_path, dbfs_tx_path)\n",
    "            print(f\"Copying {im_path} to {dbfs_im_path}\")\n",
    "            dbutils.fs.cp(im_path, dbfs_im_path)\n",
    "            \n",
    "            print(\"Files copied successfully, now reading from DBFS...\")\n",
    "            txns = spark.read.csv(dbfs_tx_path, header=True, inferSchema=True)\n",
    "            importance = spark.read.csv(dbfs_im_path, header=True, inferSchema=True)\n",
    "            \n",
    "            print(f\"Successfully loaded {txns.count()} transactions and {importance.count()} importance records from DBFS\")\n",
    "            return txns, importance\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Method 2 also failed: {e2}\")\n",
    "            print(\"Trying Method 3: Use Unity Catalog path...\")\n",
    "            \n",
    "            try:\n",
    "                # Method 3: Try to access via Unity Catalog path\n",
    "                catalog_tx_path = f\"/Volumes/your_catalog/your_schema/your_volume/{GDRIVE_TRANSACTIONS_FILENAME}\"\n",
    "                catalog_im_path = f\"/Volumes/your_catalog/your_schema/your_volume/{GDRIVE_IMPORTANCE_FILENAME}\"\n",
    "                \n",
    "                print(f\"Attempting to read from Unity Catalog: {catalog_tx_path}\")\n",
    "                txns = spark.read.csv(catalog_tx_path, header=True, inferSchema=True)\n",
    "                importance = spark.read.csv(catalog_im_path, header=True, inferSchema=True)\n",
    "                \n",
    "                print(f\"Successfully loaded from Unity Catalog: {txns.count()} transactions and {importance.count()} importance records\")\n",
    "                return txns, importance\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"Method 3 also failed: {e3}\")\n",
    "                print(\"Trying Method 4: Use workspace path...\")\n",
    "                \n",
    "                try:\n",
    "                    # Method 4: Try to access via workspace path\n",
    "                    workspace_tx_path = f\"/Workspace/Users/your_email@domain.com/{GDRIVE_TRANSACTIONS_FILENAME}\"\n",
    "                    workspace_im_path = f\"/Workspace/Users/your_email@domain.com/{GDRIVE_IMPORTANCE_FILENAME}\"\n",
    "                    \n",
    "                    print(f\"Attempting to read from workspace: {workspace_tx_path}\")\n",
    "                    txns_pd = pd.read_csv(workspace_tx_path)\n",
    "                    importance_pd = pd.read_csv(workspace_im_path)\n",
    "                    \n",
    "                    txns = spark.createDataFrame(txns_pd)\n",
    "                    importance = spark.createDataFrame(importance_pd)\n",
    "                    \n",
    "                    print(f\"Successfully loaded from workspace: {txns.count()} transactions and {importance.count()} importance records\")\n",
    "                    return txns, importance\n",
    "                    \n",
    "                except Exception as e4:\n",
    "                    print(f\"All methods failed. Final error: {e4}\")\n",
    "                    raise Exception(f\"Could not load files from any source. Errors: Method1={e}, Method2={e2}, Method3={e3}, Method4={e4}\")\n",
    "\n",
    "\n",
    "def load_from_catalog(spark: SparkSession) -> tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"Load data directly from Unity Catalog tables\"\"\"\n",
    "    print(f\"Loading from Unity Catalog tables:\")\n",
    "    print(f\"  Transactions: {CATALOG_TRANSACTIONS_PATH}\")\n",
    "    print(f\"  Importance: {CATALOG_IMPORTANCE_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        # Read directly from Unity Catalog tables\n",
    "        txns = spark.read.table(CATALOG_TRANSACTIONS_PATH)\n",
    "        importance = spark.read.table(CATALOG_IMPORTANCE_PATH)\n",
    "        \n",
    "        print(f\"Successfully loaded {txns.count()} transactions and {importance.count()} importance records from Unity Catalog\")\n",
    "        \n",
    "        # Debug: Show schemas\n",
    "        print(\"DEBUG: Transactions schema:\")\n",
    "        txns.printSchema()\n",
    "        print(\"DEBUG: Importance schema:\")\n",
    "        importance.printSchema()\n",
    "        \n",
    "        # Debug: Show sample data\n",
    "        print(\"DEBUG: Sample transactions data:\")\n",
    "        txns.show(5, truncate=False)\n",
    "        print(\"DEBUG: Sample importance data:\")\n",
    "        importance.show(5, truncate=False)\n",
    "        \n",
    "        return txns, importance\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from Unity Catalog: {e}\")\n",
    "        print(\"Trying alternative catalog path format...\")\n",
    "        \n",
    "        try:\n",
    "            # Alternative format: try with /Volumes prefix\n",
    "            alt_tx_path = f\"/Volumes/{CATALOG_TRANSACTIONS_PATH}/transactions.csv\"\n",
    "            alt_im_path = f\"/Volumes/{CATALOG_IMPORTANCE_PATH}/CustomerImportance.csv\"\n",
    "            \n",
    "            print(f\"Trying alternative path: {alt_tx_path}\")\n",
    "            txns = spark.read.csv(alt_tx_path, header=True, inferSchema=True)\n",
    "            importance = spark.read.csv(alt_im_path, header=True, inferSchema=True)\n",
    "            \n",
    "            print(f\"Successfully loaded using alternative path: {txns.count()} transactions and {importance.count()} importance records\")\n",
    "            return txns, importance\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative path also failed: {e2}\")\n",
    "            raise Exception(f\"Could not load from Unity Catalog. Errors: {e}, Alternative: {e2}\")\n",
    "\n",
    "\n",
    "def load_inputs_normalized(spark: SparkSession) -> tuple[DataFrame, DataFrame]:\n",
    "    source = INPUT_SOURCE\n",
    "    if source == \"s3\":\n",
    "        txns_pd = _read_csv_from_s3_via_boto(INPUT_TRANSACTIONS_S3)\n",
    "        importance_pd = _read_csv_from_s3_via_boto(INPUT_CUSTOMER_IMPORTANCE_S3)\n",
    "        txns = spark.createDataFrame(txns_pd)\n",
    "        importance = spark.createDataFrame(importance_pd)\n",
    "    elif source == \"workspace\":\n",
    "        txns_pd = pd.read_csv(WORKSPACE_TRANSACTIONS_PATH)\n",
    "        importance_pd = pd.read_csv(WORKSPACE_IMPORTANCE_PATH)\n",
    "        txns = spark.createDataFrame(txns_pd)\n",
    "        importance = spark.createDataFrame(importance_pd)\n",
    "    elif source == \"volume\":\n",
    "        txns = spark.read.csv(TRANSACTIONS_CSV_VOLUME, header=True, inferSchema=True)\n",
    "        importance = spark.read.csv(CUSTOMER_IMPORTANCE_CSV_VOLUME, header=True, inferSchema=True)\n",
    "    elif source == \"catalog\":\n",
    "        txns, importance = load_from_catalog(spark)\n",
    "    elif source == \"gdrive\":\n",
    "        txns, importance = load_from_gdrive(spark)\n",
    "    else:  # dbfs\n",
    "        txns = spark.read.csv(TRANSACTIONS_CSV_DBFS, header=True, inferSchema=True)\n",
    "        importance = spark.read.csv(CUSTOMER_IMPORTANCE_CSV_DBFS, header=True, inferSchema=True)\n",
    "\n",
    "    txns = normalize_txns(txns)\n",
    "    importance = normalize_importance(importance)\n",
    "    return txns, importance\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Pattern Detection (PatId1)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class PatternDetector:\n",
    "    def __init__(self):\n",
    "        self.ist_start = datetime.now(IST)\n",
    "        self.counter = 0\n",
    "\n",
    "    def _create_detection(self, customer_id: str, merchant_id: str) -> dict:\n",
    "        self.counter += 1\n",
    "        return {\n",
    "            \"YStartTime\": self.ist_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"detectionTime\": datetime.now(IST).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"patternId\": \"PatId1\",\n",
    "            \"ActionType\": \"UPGRADE\",\n",
    "            \"customerName\": customer_id or \"\",\n",
    "            \"MerchantId\": merchant_id or \"\",\n",
    "        }\n",
    "\n",
    "    def detect_patid1(self, spark: SparkSession, txn_df: DataFrame, importance_df: DataFrame) -> List[dict]:\n",
    "        if txn_df is None or txn_df.limit(1).count() == 0:\n",
    "            print(\"DEBUG: Transaction DataFrame is empty or None\")\n",
    "            return []\n",
    "        if importance_df is None or importance_df.limit(1).count() == 0:\n",
    "            print(\"DEBUG: Importance DataFrame is empty or None\")\n",
    "            return []\n",
    "\n",
    "        print(f\"DEBUG: Starting detection with {txn_df.count()} transactions and {importance_df.count()} importance records\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"DEBUG: Sample transactions:\")\n",
    "        txn_df.show(5, truncate=False)\n",
    "        print(\"DEBUG: Sample importance:\")\n",
    "        importance_df.show(5, truncate=False)\n",
    "\n",
    "        cust_stats = (\n",
    "            txn_df.groupBy(\"merchant_id\", \"customer_id\")\n",
    "            .agg(spark_count(lit(1)).alias(\"total_transactions\"))\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG: Customer stats:\")\n",
    "        cust_stats.show(5, truncate=False)\n",
    "\n",
    "        merchant_totals = (\n",
    "            cust_stats.groupBy(\"merchant_id\")\n",
    "            .agg(spark_sum(\"total_transactions\").alias(\"merchant_total_txns\"))\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG: Merchant totals:\")\n",
    "        merchant_totals.show(5, truncate=False)\n",
    "\n",
    "        w = Window.partitionBy(\"merchant_id\").orderBy(desc(\"total_transactions\"))\n",
    "        cust_ranked = cust_stats.withColumn(\"tile10\", ntile(10).over(w))\n",
    "        top10 = cust_ranked.filter(col(\"tile10\") == 1)\n",
    "        \n",
    "        print(\"DEBUG: Top 10% customers:\")\n",
    "        top10.show(5, truncate=False)\n",
    "\n",
    "        imp_avg = (\n",
    "            importance_df.groupBy(\"merchant_id\", \"customer_id\")\n",
    "            .agg(avg(\"weightage\").alias(\"avg_weightage\"))\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG: Importance averages:\")\n",
    "        imp_avg.show(5, truncate=False)\n",
    "\n",
    "        merchant_thresholds = (\n",
    "            imp_avg.groupBy(\"merchant_id\")\n",
    "            .agg(expr(\"percentile_approx(avg_weightage, 0.10)\").alias(\"w10\"))\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG: Merchant thresholds (10th percentile):\")\n",
    "        merchant_thresholds.show(5, truncate=False)\n",
    "\n",
    "        top10_with_weight = (\n",
    "            top10.join(imp_avg, [\"merchant_id\", \"customer_id\"], \"left\")\n",
    "            .join(merchant_totals, [\"merchant_id\"], \"left\")\n",
    "            .join(merchant_thresholds, [\"merchant_id\"], \"left\")\n",
    "        )\n",
    "        \n",
    "        print(\"DEBUG: Top 10% with weights and thresholds:\")\n",
    "        top10_with_weight.show(5, truncate=False)\n",
    "\n",
    "        eligible = top10_with_weight.where(\n",
    "            (col(\"merchant_total_txns\") >= lit(MIN_TRANSACTIONS_FOR_UPGRADE))\n",
    "            & (col(\"avg_weightage\").isNotNull())\n",
    "            & (col(\"w10\").isNotNull())\n",
    "            & (col(\"avg_weightage\") <= col(\"w10\"))\n",
    "        ).select(\"merchant_id\", \"customer_id\")\n",
    "        \n",
    "        print(f\"DEBUG: MIN_TRANSACTIONS_FOR_UPGRADE = {MIN_TRANSACTIONS_FOR_UPGRADE}\")\n",
    "        print(\"DEBUG: Eligible customers after all filters:\")\n",
    "        eligible.show(5, truncate=False)\n",
    "        \n",
    "        # Additional debugging: Show merchants that meet the threshold\n",
    "        print(\"DEBUG: Merchants that meet the transaction threshold:\")\n",
    "        merchants_above_threshold = merchant_totals.filter(col(\"merchant_total_txns\") >= lit(MIN_TRANSACTIONS_FOR_UPGRADE))\n",
    "        merchants_above_threshold.show(10, truncate=False)\n",
    "        \n",
    "        # Show how many customers would be eligible if we ignored the transaction threshold\n",
    "        print(\"DEBUG: Customers that would be eligible without transaction threshold:\")\n",
    "        eligible_without_threshold = top10_with_weight.where(\n",
    "            (col(\"avg_weightage\").isNotNull())\n",
    "            & (col(\"w10\").isNotNull())\n",
    "            & (col(\"avg_weightage\") <= col(\"w10\"))\n",
    "        ).select(\"merchant_id\", \"customer_id\")\n",
    "        eligible_without_threshold.show(5, truncate=False)\n",
    "\n",
    "        rows = eligible.distinct().collect()\n",
    "        print(f\"DEBUG: Final detections: {len(rows)}\")\n",
    "        return [self._create_detection(r[\"customer_id\"], r[\"merchant_id\"]) for r in rows]\n",
    "\n",
    "    def batch(self, detections: List[dict]):\n",
    "        for i in range(0, len(detections), DETECTION_BATCH_SIZE):\n",
    "            yield detections[i : i + DETECTION_BATCH_SIZE]\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Mechanism X (chunk to S3) and Y (poll S3, detect, write)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def run_mechanism_x() -> List[str]:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    s3 = S3Client()\n",
    "    s3.create_bucket_if_not_exists()\n",
    "\n",
    "    txns, _ = load_inputs_normalized(spark)\n",
    "\n",
    "    total = int(txns.count())\n",
    "    uploaded_keys: List[str] = []\n",
    "    txns_pd = txns.toPandas()\n",
    "\n",
    "    chunk_index = 0\n",
    "    for start in range(0, total, CHUNK_SIZE):\n",
    "        end = min(start + CHUNK_SIZE, total)\n",
    "        chunk_pd = txns_pd.iloc[start:end]\n",
    "        chunk_id = f\"chunk_{chunk_index}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        key = s3.upload_transaction_chunk(chunk_pd, chunk_id)\n",
    "        uploaded_keys.append(key)\n",
    "        print(f\"X: uploaded {key}\")\n",
    "        chunk_index += 1\n",
    "        time.sleep(X_SLEEP_SECONDS)\n",
    "    return uploaded_keys\n",
    "\n",
    "\n",
    "def run_mechanism_y_once(keys: Optional[List[str]] = None):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    s3 = S3Client()\n",
    "\n",
    "    _, importance = load_inputs_normalized(spark)\n",
    "\n",
    "    if not keys:\n",
    "        keys = s3.list_keys(S3_TRANSACTIONS_FOLDER)\n",
    "\n",
    "    detector = PatternDetector()\n",
    "    processed_now: List[str] = []\n",
    "\n",
    "    for key in keys:\n",
    "        obj = s3.s3_client.get_object(Bucket=s3.bucket, Key=key)\n",
    "        chunk_pd = pd.read_csv(io.StringIO(obj[\"Body\"].read().decode(\"utf-8\")))\n",
    "        chunk_spark = spark.createDataFrame(chunk_pd)\n",
    "\n",
    "        dets = detector.detect_patid1(spark, chunk_spark, importance)\n",
    "        if dets:\n",
    "            for i, batch in enumerate(detector.batch(dets)):\n",
    "                s3.upload_detections_batch(batch, f\"{key.replace('/', '_')}_{i}\")\n",
    "        print(f\"Y: processed {key}, detections={len(dets)}\")\n",
    "        processed_now.append(key)\n",
    "\n",
    "    if processed_now and pg_available():\n",
    "        pg_mark_keys_processed(processed_now)\n",
    "\n",
    "\n",
    "def run_mechanism_y_streaming():\n",
    "    s3 = S3Client()\n",
    "    seen: Set[str] = set()\n",
    "\n",
    "    if pg_available():\n",
    "        pg_ensure_tables()\n",
    "        seen = pg_load_processed_keys()\n",
    "        print(f\"Loaded {len(seen)} processed keys from Postgres\")\n",
    "\n",
    "    rounds = 0\n",
    "    while rounds < Y_MAX_POLL_ROUNDS:\n",
    "        rounds += 1\n",
    "        all_keys = s3.list_keys(S3_TRANSACTIONS_FOLDER)\n",
    "        new_keys = [k for k in all_keys if k not in seen]\n",
    "        if new_keys:\n",
    "            print(f\"Y round {rounds}: found {len(new_keys)} new keys\")\n",
    "            run_mechanism_y_once(new_keys)\n",
    "            seen.update(new_keys)\n",
    "        else:\n",
    "            print(f\"Y round {rounds}: no new keys\")\n",
    "        time.sleep(Y_POLL_INTERVAL_SECS)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## Execute\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(f\"INPUT_SOURCE={INPUT_SOURCE}\")\n",
    "\n",
    "uploaded = None\n",
    "\n",
    "if RUN_MODE == \"both\":\n",
    "    # Start Y streaming concurrently, then run X\n",
    "    y_thread = threading.Thread(target=run_mechanism_y_streaming, daemon=True)\n",
    "    y_thread.start()\n",
    "    uploaded = run_mechanism_x()\n",
    "    # After X finishes, allow a couple more rounds for Y to finish up\n",
    "    time.sleep(max(2 * Y_POLL_INTERVAL_SECS, 2))\n",
    "elif RUN_MODE == \"x\":\n",
    "    uploaded = run_mechanism_x()\n",
    "elif RUN_MODE == \"y\":\n",
    "    run_mechanism_y_streaming()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#  %md\n",
    "#  ## List outputs in S3\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "s3 = S3Client()\n",
    "resp_tx = s3.s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_TRANSACTIONS_FOLDER)\n",
    "print(\"Transactions in S3:\")\n",
    "for o in (resp_tx.get(\"Contents\", [])[:10]):\n",
    "    print(\" -\", o[\"Key\"])\n",
    "\n",
    "resp_det = s3.s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_DETECTIONS_FOLDER)\n",
    "print(\"\\nDetections in S3:\")\n",
    "for o in (resp_det.get(\"Contents\", [])[:10]):\n",
    "    print(\" -\", o[\"Key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "014b1197-3f5b-4cfd-b500-453bddfee480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transaction Processing to s3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}